data:
  corpus:                                 # Pass to dataloader
    # The following depends on corpus
    name: 'PTS'                   # Specify corpus
    path: '/home/nlpmaster/ssd-1t/corpus/PTS-MSub-Vol1/segmented' #'pth/to/libri/speech'
    train_split: ['train']
    dev_split: ['dev']
    bucketing: True
    batch_size: 8

  audio:                                  # Pass to audio transform
    feat_type: 'fbank'
    feat_dim:  80
    apply_cmvn: True
    delta_order: 0                        # 0: do nothing, 1: add delta, 2: add delta and accelerate
    delta_window_size: 2
    frame_length: 25 # ms
    frame_shift: 10 # ms
    dither: 0 # random dither audio, 0: no dither

  text:
    mode: 'word'                     # 'character'/'word'/'subword'
    vocab_file: 'tests/sample_data/pts.vocab'

hparas:                                   # Experiment hyper-parameters
  valid_step: 5000
  max_step: 200000
  tf_start: 1.0
  tf_end: 1.0
  tf_step: 150000
  optimizer: 'Adadelta'
  lr: 1.0
  eps: 0.00000001                          # 1e-8
  lr_scheduler: 'fixed'                    # 'fixed'/'warmup'
  
model:                                     # Model architecture
  ctc_weight: 0.5                          # Weight for CTC loss
  encoder:
    vgg: True                              # 4x reduction on time feature extraction
    module: 'LSTM'                         # 'LSTM'/'GRU'/'Transformer'
    bidirection: False
    dim: [320,320,320,320,320]
    dropout: [0,0,0,0,0]
    layer_norm: [False,False,False,False,False]
    proj: [True,True,True,True,True]       # Linear projection + Tanh after each rnn layer
    sample_rate: [1,1,1,1,1]
    sample_style: 'drop'                   # 'drop'/'concat'
  attention:
    mode: 'mocha'                            # 'dot'/'loc'
    dim: 300
    num_head: 2
    v_proj: False                          # if False and num_head>1, encoder state will be duplicated for each head
    temperature: 0.5                        # scaling factor for attention
    loc_kernel_size: 100                   # just for mode=='loc'
    loc_kernel_num: 10 
    chunk_size: 6
  decoder:
    module: 'LSTM'                         # 'LSTM'/'GRU'/'Transformer'
    dim: 320
    layer: 1
    dropout: 0
    layer_norm: False
